---
title: "MLM Final Project Part 2ab"
date:  "`r format(Sys.time(), '%B %d %Y')`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
###Load libraries
if(!requireNamespace("here"))
  install.packages("here", repos = "https://cloud.r-project.org")
require("here")

if(!requireNamespace("ggplot2"))
  install.packages("ggplot2", repos = "https://cloud.r-project.org")
require("ggplot2")

if(!requireNamespace("lme4"))
  install.packages("lme4", repos = "https://cloud.r-project.org")
require("lme4")

if(!requireNamespace("lmerTest"))
  install.packages("lmerTest", repos = "https://cloud.r-project.org")
require("lmerTest")

if(!requireNamespace("car"))
  install.packages("car", repos = "https://cloud.r-project.org")
require("car")

install.packages("gatepoints")
library("gatepoints")

vanillaR <- F
    showR <- T
```

```{r, tidy = TRUE}
dat <- read.csv("classroom.csv")

dat$math1st <- dat$mathkind + dat$mathgain
```


## Team Members and division of work: 

## Question 1
Refit the model in Part 1 that has all fixed effects as well as random intercepts (in schools and classrooms). Recall that `math1st = mathkind + mathgain` is the outcome. The model is `math1st ~ housepov + yearstea + mathprep + mathknow + ses + sex + minority + (1|schoolid/classid), REML = T)`

```{r, tidy = TRUE}
# Insert code to fit model and print summary

fit1 <- lmerTest::lmer(math1st ~ housepov + yearstea + mathprep + mathknow + ses + sex + minority + (1|schoolid/classid), data = dat, REML = T) 
```

a. Construct the residual that removes only the 'fixed effects' then subtract it from the outcome; call this residual resFE
    i. R hint 1: `predict` has an option to generate the prediction based on the fixed effects only. 
    ii. R hint 2: If you decide to add a column to your data frame with resFE, note that predict only generates predictions for cases uses in the model *after listwise deletion.* 

```{r, tidy = TRUE}
# Insert code here to construct residual 

yhat <- predict(fit1, re.form = ~0)
dat$resFE[complete.cases(dat)] <- dat$math1st[complete.cases(dat)] - yhat

```

## Question 2 
Show that the residual is not independent within schools in some manner. 
```{r}
# Insert code to show that the residual, resFE, is not independent within schools

ord <- order(unlist(tapply(dat$resFE, dat$schoolid, median)))
if (vanillaR) {
boxplot(split(dat$resFE, dat$schoolid)[ord])
} else {
ggplot(dat[complete.cases(dat),], aes(x = reorder(schoolid, resFE, FUN = median), y = resFE)) +
geom_boxplot()
}

```

## Question 3
### a. Construct the residual that utilizes the BLUPs for the random effects using the R command `residuals`.
    
    i. Call the new residual resFE_RE
    
```{r, tidy = TRUE}
# Insert code to construct the residual

save.options <- options()
options(na.action = "na.pass")
mm <- model.matrix(~math1st + housepov + yearstea + mathprep + mathknow + ses + sex + minority, data = dat)
options(save.options)
in_sample <- apply(is.na(mm), 1, sum) == 0
dat$resFE_RE <- rep(NA, dim(dat)[1]) 
dat$resFE_RE[in_sample] <- residuals(fit1)

```

## Question 4
### a. Show that these new residuals, resFE_RE are MUCH LESS (if not completely un-) correlated within schools, using the same method as before (boxplot?) **(you should comment)** 
```{r, tidy = TRUE}
# Insert code to show that new residuals, resFE_RE, is much less correlated within schools

ord <- order(unlist(tapply(dat$resFE_RE, dat$schoolid, median)))
if (vanillaR) {
boxplot(split(dat$resFE_RE, dat$schoolid)[ord])
} else {
ggplot(dat[complete.cases(dat),], aes(x = reorder(schoolid, resFE_RE, FUN = median), y = resFE_RE)) +
geom_boxplot()
}
```

    Response: The new residuals, resFE_RE, are much less correlated within school. This is because the boxplots are more centered around zero (median for each school would be much closer to zero if the errors are independent).

## Question 5
### a. Generate the two sets of BLUPs (for random effects zeta0 and eta0)
```{r, tidy = TRUE}
# Insert code to generate the two sets of BLUPS (zeta0 and eta0)

ranefs <- ranef(fit1)
zeta0 <- ranefs$schoolid[, 1]
eta0 <- ranefs$classid[, 1]

```

### b. Examine these for normality (include evidence), **and comment**.
```{r, tidy = TRUE}
# Insert code to examine BLUPs for normality
# par(mfrow=c(1,2)) produces palette for one row of plots with two columns 

# Examine zeta0
par(mfrow=c(1,2))
plot(density(zeta0))
qqnorm(zeta0)
qqline(zeta0)

# Examine eta0
par(mfrow=c(1,2))
plot(density(eta0))
qqnorm(eta0)
qqline(eta0)
```

    Response: It appears that the BLUPs for classroom effects are fairly normal. Their density plot has a bell-shaped, symmetric distribution and their Q-Q plot has most of the points falling about the straight line. The BLUPs for school effects appear slightly less normal. Their density plot has a less symmetric distribution and the points on their Q-Q plot deviate a bit more from forming a straight line.   

## Question 6 
### a. Fit a slightly more complicated model with the same fixed effects, but now add a random slope for minority, correlated with the random intercept, at the school level (keep the classroom level random intercept).
```{r}
# Insert code to fit the slightly more complicated model and print the summary
fit2 <- lmerTest::lmer(math1st ~ housepov + yearstea + mathprep + mathknow + ses + sex + minority + (minority|schoolid) + (1|schoolid:classid), data = dat)
print(summary(fit2))
```

### b. Construct the residual (individual, level 1) and the BLUPs for the remaining random effects. Call the new residual resFE_RE as before.
```{r}
# Insert code to construct residual and BLUPs

# Residual
resFE_RE <- residuals(fit2)

# BLUPs
ranefs.fit2 <- ranef(fit2)
eta0.fit2 <- ranefs.fit2$'schoolid:classid'[, 1]
zeta0.fit2 <- ranefs.fit2$schoolid[, 1]
zeta1.fit2 <- ranefs.fit2$schoolid[,2]
```

### c. Examine all error estimates (individual level residuals, BLUPs (school and classroom level) for normality **(and comment)**). 
```{r}
# Insert code to examine error estimates.

# Examine eta0
par(mfrow=c(1,2))
plot(density(resFE_RE))
qqnorm(resFE_RE)
qqline(resFE_RE)

# Examine eta0
par(mfrow=c(1,2))
plot(density(eta0.fit2))
qqnorm(eta0.fit2)
qqline(eta0.fit2)

# Examine zeta0
par(mfrow=c(1,2))
plot(density(zeta0.fit2))
qqnorm(zeta0.fit2)
qqline(zeta0.fit2)

# Examine zeta1
par(mfrow=c(1,2))
plot(density(zeta1.fit2))
qqnorm(zeta1.fit2)
qqline(zeta1.fit2)

```

    Response: It appears that the individual level residuals and BLUPs for classroom effects are fairly normal. Their density plots have a bell-shaped, symmetric distribution and their Q-Q plots have most of the points falling about the straight line. The BLUPs for school effects (both the random slope for minority and the random intercept) are less normal. Their density plots have a less symmetric distribution and the points on their Q-Q plots deviate more from forming a straight line.   

### d. Plot zeta0 vs. zeta1 to see whether the estimated correlation is consistent with the observed. **Briefly comment**. 
```{r}
# Insert code for plot and estimate correlation
plot(zeta0.fit2, zeta1.fit2, main="Zeta0 vs. Zeta1",
   xlab="zeta1", ylab="zeta0", pch=19)
abline(lm(zeta0.fit2~zeta1.fit2), col="red")
abline(7,0)
abline(-6,0)
abline(v=-11)
abline(v=12)
```

    Response: The estimated correlation is -0.83 but it is not consistent with some of the points in the scatterplot that appear to be positively correlated.   


### e. Track down those odd points in the scatterplot. What schools are they? Do they have anything in common? **(You should comment)**
```{r}
# Insert code if you want to examine odd points 

zeta1_sub <- zeta1.fit2[which(zeta1.fit2 >= -11 & zeta1.fit2 <= 12 & zeta0.fit2 >= -6 & zeta0.fit2 <= 7)]
zeta0_sub <- zeta0.fit2[which(zeta1.fit2 >= -11 & zeta1.fit2 <= 12 & zeta0.fit2 >= -6 & zeta0.fit2 <= 7)]
dat_plot <- data.frame(zeta1_sub,zeta0_sub)

X11()
plot(dat_plot, main="Zeta0 vs. Zeta1",
   xlab="zeta1", ylab="zeta0", pch=19, col = "red")

selectedPoints <- fhs(dat_plot, mark = TRUE)
print(selectedPoints)

zeta1_sub_odd <- zeta1_sub[-c(8,19,21,22,25,29,31,35,36,39,47:50,54,57,58,59)]
zeta0_sub_odd <- zeta0_sub[-c(8,19,21,22,25,29,31,35,36,39,47:50,54,57,58,59)]

odd_points <- data.frame(zeta0_sub_odd,zeta1_sub_odd)
odd_points


```

    Response: The odd points are from schoolids 1, 4, 5, 9, 10, 12, 14, 16, 17, 19, 20, 22, 23, 24, 26, 28, 33, 38, 42, 43, 45, 46, 47, 52, 57, 60, 61, 69, 73, 78, 79, 80, 84, 86, 89, 90, 96, 98, 100, 102, and 103. It appears that they all have MINORITY=0 in common.

## Question 7
Make a *person-period* file with math score (Kindergarten and First grade). That is, `math0 <- mathkind; math1 <- mathkind + mathgain` (you have to make this work in the dataframe). Using `reshape` in R, you have to be careful to specify the name of the math variable (`math0` and `math1`) as *varying*. 

```{r}
# Insert code to create the variables math0 and math1 and to reshape data
dat$math0 <- dat$mathkind
dat$math1 <- dat$mathkind+dat$mathgain
class_pp <- reshape(dat, varying = c("math0", "math1"), v.names = "math", timevar = "year",
times = c(0, 1), direction = "long")
```


## Question 8
We ignore classrooms in this analysis, but keep it in the notation. 

### a. Fit a model with `math` as outcome, and fixed effect for time trend (`year`), and random intercepts for schools.
```{r}
# Insert code to fit model and print summary
fit.M00 <- lmer(math ~ year + (1 | schoolid/childid), data = class_pp)
print(summary(fit.M00))
```

### b. Write down the model

    Equation:
    
$$
MATH_{tijk} = {b_0} + {\delta_{0ijk}} + {\zeta_{0k}} + {b_1}TIM{E_{tijk}} + {\varepsilon_{tijk}},
$$
and we assume ${\delta_{0ijk}} \sim N(0,\sigma_{{\delta_0}}^2)$, ${\zeta_{0k}} \sim N(0,\sigma_{{\zeta_0}}^2)$, and ${\varepsilon_{tijk}} \sim N(0,\sigma _\varepsilon ^2)$
    
### c. Add random intercepts for child
```{r}
# Insert code to fit new model and print summary output
fit.M0 <- lmer(math ~ year + (1 | schoolid) + (1 | schoolid:classid) + (1 |
classid:childid), data = class_pp)
summary(fit.M0)
```

### d. Write down the model

    Equation: 

$$
MATH_{tijk} = {b_0} + {\delta_{0ijk}} + {\eta_{0jk}} + {\zeta_{0k}} + {b_1}TIME_{tijk} + {\varepsilon_{tijk}}
$$
and assume ${\delta_{0ijk}} \sim N(0,\sigma_{{\delta_0}}^2)$, ${\eta_{0jk}} \sim N(0,\sigma_{{\eta_{00}}}^2)$, ${\zeta_{0k}} \sim N(0,\sigma_{{\zeta_0}}^2)$, and ${\varepsilon_{tijk}} \sim N(0,\sigma _\varepsilon ^2)$, independently.

## Question 9
Report original and new variance estimates of $\sigma^2_{\zeta_0}$ (between schools) and $\sigma^2_{\varepsilon}$ (within schools):

$\sigma^2_{\zeta_0}:$ The original variance estimate is 307.5. The new variance estimate is 294.31.
$\sigma^2_{\varepsilon}:$ The original variance estimate is 599.1. The new variance estimate is 599.05.
    
### a. Compute a pseudo $R^2$ relating the between school variation and ignoring between students in the same school. In other words, what fraction of the between-school variance in the first model is 'explained' by the addition of a student random effect?
```{r}
# Insert code to compute psuedo R^2 or do this inline 
(307.5-294.31)/307.5
```
    Response: The proportion of the between-school variance in the first model that is ‘explained’ by the addition of a student random effect is 0.04.
    
### b. Does the total variation stay about the same (adding between children within schools variance as well, to the second model results) **(you should comment)**?
```{r}
# Total variation of first model
702+307.5+599.1

# Total variaion of second model
675.98+39.01+294.31+599.05
```

    Response: Yes, the total variation stays about the same. The total variation of the first model is 1608.6. The total variation of the second model is 1608.35.
    
## Question 10 
Add a random slope ($\zeta_1$) for the trend (year) within schools (uncorrelated with random intercept ($\zeta_0$))
```{r}
# Insert code to fit model and print out summary 
fit.M1 <- lmer(math ~ year + (year || schoolid) + (1 | schoolid:classid) + (1 |
classid:childid), data = class_pp)
summary(fit.M1)
```


### a. Generate the BLUPs for the random effects and examine whether the independence between zeta_0 and zeta_1 is reflected in a scatterplot of these two sets of effects. **(you should comment)**
```{r}
# Insert code to generate BLUPs
ranefs.fit.M1 <- ranef(fit.M1)
delta.fit.M1 <- ranefs.fit.M1$`classid:childid`[,1]
eta0.fit.M1 <- ranefs.fit.M1$'schoolid:classid'[, 1]
zeta0.fit.M1 <- ranefs.fit.M1$schoolid[,2]
zeta1.fit.M1 <- ranefs.fit.M1$schoolid[,1]

# Scatterplot of zeta_0 vs. zeta_1
plot(zeta0.fit.M1, zeta1.fit.M1, main="Zeta0 vs. Zeta1",
   xlab="zeta1", ylab="zeta0", pch=19)

```

    Response: Yes, the independence of zeta_0 and zeta_1 is reflected in the scatterplot because there does not seem to be any correlation between the two.

### b. Compute V_S(year = 0) and V_S (year = 1). Since there are only two years, this is a form of heteroscedasticity in the random effects.
```{r}
# Insert code to compute terms or do this inline 
V_S_year0 = 312.12
V_S_year0

V_S_year1 = 312.12 + 88.57
V_S_year1
```

i. In which year is there more between school variation, net of all else, **(you should comment)**?
    
    Response: There is more between school variation in year 1. 

## Question 11
If you ran the model BY YEAR, and removed the year trend from the model, would you get the same estimates for the variances between schools? **(you should comment)* *
```{r}
# Insert code to fit the two models by year and print out the summary 
fit.M_K <- lmer(mathkind ~ 1 + (1 | schoolid/classid), data = dat)
print(summary(fit.M_K))

fit.M_1 <- lmer(math1st ~ 1 + (1 | schoolid/classid), data = dat)
print(summary(fit.M_1))
```

    Response: No, you get different estimates for the variances between schools.

## Question 12 
Rerun the last nested longitudinal model, allowing correlation between intercept and slope.

### a. Is the correlation significant? **(you should comment)**
```{r}
# Insert code to fit model, print the summary output, and compare models
fit.M2 <- lmer(math ~ year + (year | schoolid) + (1 | schoolid:classid) + (1 |
classid:childid), data = class_pp)
summary(fit.M2)

# LRT
anova(fit.M2, fit.M1, refit = F)
```

    Response: Yes, the LRT suggests that the correlation is significant.

### b. Compute V_S (year = 0) and V_S(year = 1) for this new model (your formula should include covariance terms).
```{r}
# Insert code to compute terms or do this inline 
V_S_year0_new = 357.41
V_S_year0_new

V_S_year1_new = 357.41 + 2*(-0.46)*10.442*18.905 + 109.03
V_S_year1_new
```

  i. Is this result (and thus model) more consistent with the separate grade analysis? You are implicity testing model fit here. **(you should comment)**
      
      Response: Yes, this result (and model) is more consistent with the separate grade analysis. 






